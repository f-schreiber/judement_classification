{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6636d1ae-2c68-49b1-8292-27cbb26e2e94",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: openpyxl in /opt/conda/lib/python3.10/site-packages (3.1.2)\n",
      "Requirement already satisfied: et-xmlfile in /opt/conda/lib/python3.10/site-packages (from openpyxl) (1.1.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: langchain_text_splitters in /opt/conda/lib/python3.10/site-packages (0.3.6)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /opt/conda/lib/python3.10/site-packages (from langchain_text_splitters) (0.3.35)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /opt/conda/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (0.3.8)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (6.0)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (2.10.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (3.10.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (2.31.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (2.27.2)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (3.6.2)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (2023.5.7)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (1.0.5)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (3.4)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain_text_splitters) (2.2.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.48.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.0.0+cu117)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.48.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.65.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.29.3)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.28.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2023.5.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.0.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.13.0->peft) (3.25.0)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.13.0->peft) (15.0.7)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.4.16)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install openpyxl\n",
    "!pip install langchain_text_splitters\n",
    "!pip install -U transformers\n",
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c2a2c1-34d7-4a53-ad19-1c5800586ad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import tqdm\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "385e2ffe-c9c3-44c0-b94a-057f937c9d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dadefb5e-7bc7-4550-85f8-ee067f09fc07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc71ecb0-d017-4642-b432-f63419c77b32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_pipeline(model_name):\n",
    "    import torch\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "    from peft import PeftModel\n",
    "    if \"lora\" in model_name:\n",
    "        base_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "        lora_model_path = model_name.split(\"-\")[1]\n",
    "\n",
    "        # Load base model and tokenizer\n",
    "        model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "        # Load LoRA adapter\n",
    "        model = PeftModel.from_pretrained(model, lora_model_path)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if tokenizer.pad_token is None: # Padding-Token\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        # Modell laden\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "    # Pipeline\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,  \n",
    "        max_new_tokens = 200, # max. generierte Antwort\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5575f86-c035-467f-84c4-f59bba2b75f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def question_answer(pipe, text, question):\n",
    "    prompt = f\"Beantworte kurz die Frage \\n{question} basierend auf folgendem Text mit ja oder nein: \\n{text}\" \n",
    "    result = pipe(prompt)[0]['generated_text']\n",
    "    return result[len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a36db08-6f69-4bdb-acdf-8efe1d625d16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(file_path, model_name, pipeline, question):\n",
    "    text = read_text(file_path)\n",
    "    pipe = update_pipeline(model_name)\n",
    "    summary = summarize(text, pipe)\n",
    "    answer = question_answer(pipe, text, question)\n",
    "    evaluation = evaluate_answer(answer)\n",
    "    return summary, answer, evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88975971-586d-4399-a1c3-030213993d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"Sample_combined.xlsx\")\n",
    "#data = data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95340841-502f-4e43-874a-6474ecb66d56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = \"Gerichtsurteile/BAG/jb-KARE600028845.txt\"\n",
    "model_name = \"mistralai/Mistral-Small-24B-Instruct-2501\"\n",
    "question1 = \"Werden in dem Urteil Zahlungsstreitigkeiten behandelt?\"\n",
    "question2 = \"Behandelt das Urteil den Arbeitsschutz?\"\n",
    "question3 = \"Wird eine Stafttat gegen die körperliche Unversehrheit betrachtet?\"\n",
    "question4 = \"Ist der Streitgegenstand ein Patentsachverhalt?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94b729fc-7eb0-4ab6-85b9-43b27262e7aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Auswertung der Antwort\n",
    "def evaluate_answer(result):\n",
    "    result_lower = result.lower()\n",
    "    if \"ja\" in result_lower and \"nein\" not in result_lower:\n",
    "        return 1\n",
    "    if \"nein\" in result_lower and \"ja\" not in result_lower:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "#generate(file_path, model_name, pipeline, question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce00f124-136f-4ad1-b061-46167c1bc18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 08:35:02.391311: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91917bc656ea45fe9953886aa0328d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d1540060264b7d8bf6822bea488daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "questions = [\"Werden in dem Urteil Zahlungsstreitigkeiten behandelt?\",\n",
    "             \"Behandelt das Urteil den Arbeitsschutz?\",\n",
    "             \"Wird eine Stafttat gegen die körperliche Unversehrheit betrachtet?\",\n",
    "             \"Ist der Streitgegenstand ein Patentsachverhalt?\"]\n",
    "\n",
    "model_list = [\"mistralai/Mistral-Small-24B-Instruct-2501\",\"mistralai/Mistral-7B-Instruct-v0.2\",\"lora-results_2m\",\"lora-results_4m\"]\n",
    "\n",
    "results_dict = dict()\n",
    "for model in model_list:\n",
    "    #pipeline festlegen\n",
    "    pipeline = update_pipeline(model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    model_results = list()\n",
    "    \n",
    "    for i in data.itertuples():\n",
    "        try:\n",
    "            content = read_text(\"Sample_1/\"+i[2])\n",
    "        except FileNotFoundError:\n",
    "            content = read_text(\"Sample_2/\"+i[2])\n",
    "\n",
    "        token_count = len(tokenizer.encode(content))\n",
    "            \n",
    "        max_tokens = 4000\n",
    "        if token_count > max_tokens:\n",
    "            text_splitter = RecursiveCharacterTextSplitter(separators = [\"\\n\\n\",\"\\n\",\".\"],chunk_size = 2000,chunk_overlap = 0)\n",
    "            split_texts = text_splitter.split_text(content)\n",
    "            \n",
    "            results = []\n",
    "            #print(\"here is split text\")\n",
    "            for question in questions:\n",
    "                split_results = []\n",
    "                for part in split_texts:\n",
    "                    answer = question_answer(pipeline,part,question)\n",
    "\n",
    "                    if \"yes\" in answer.lower()[:6]+answer.lower()[-6:] or \"ja\" in answer.lower()[:6]+answer.lower()[-6:]:\n",
    "                        split_results.append(1)\n",
    "                    elif \"no\" in answer.lower()[:6]+answer.lower()[-6:] or \"nein\" in answer.lower()[:6]+answer.lower()[-6:]:\n",
    "                        split_results.append(0)\n",
    "                    else:\n",
    "                        split_results.append(2)\n",
    "                \n",
    "                if 1 in split_results:\n",
    "                    results.append(1)\n",
    "                elif 0 in split_results:\n",
    "                    results.append(0)\n",
    "                else:\n",
    "                    results.append(2)\n",
    "            #print(results)\n",
    "\n",
    "\n",
    "        else:\n",
    "            results = []\n",
    "            for question in questions:\n",
    "                answer = question_answer(pipeline,content,question)\n",
    "                \n",
    "                if \"yes\" in answer.lower()[:6]+answer.lower()[-6:] or \"ja\" in answer.lower()[:6]+answer.lower()[-6:]:\n",
    "                    results.append(1)\n",
    "                elif \"no\" in answer.lower()[:6]+answer.lower()[-6:] or \"nein\" in answer.lower()[:6]+answer.lower()[-6:]:\n",
    "                    results.append(0)\n",
    "                else:\n",
    "                    results.append(2)\n",
    "        \n",
    "        model_results.append(results)\n",
    "        print(i[0])\n",
    "                \n",
    "                \n",
    "    results_dict[model] = model_results.copy()            \n",
    "    del pipeline \n",
    "    time.sleep(15)\n",
    "    #pipeline löschen\n",
    "    #15s warten -- speicher freimachen\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162e4560-dbda-4a67-9cc9-7f1fa390355b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_names = {\"lora-results_2m\" : \"2m_non_summarized.csv\",\n",
    "              \"lora-results_4m\" : \"4m_non_summarized.csv\",\n",
    "              \"mistralai/Mistral-7B-Instruct-v0.2\" : \"7b-instruct_no_summary.csv\",\n",
    "              \"mistralai/Mistral-Small-24B-Instruct-2501\": \"24b-instruct_no_summary.csv\"}\n",
    "\n",
    "for i in results_dict.keys():\n",
    "    pd.DataFrame(results_dict[i]).to_csv(\"no_summary_results/\"+file_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16de6064-639a-4b79-8009-a27c11c9c7f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f77798e-72a2-4c0d-a8d0-33042807d81e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d52ddc9-abbe-4d33-9b84-91117a4ec38a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fb40f4-51a2-4264-b972-542e5d03dc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(results_dicts[\"mistralai/Mistral-Small-24B-Instruct-2501\"]).to_csv(\"27bMistral_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b04e12-c7af-4029-8140-b85dc746b916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if \"yes\" or \"ja\" in answer.lower()[0:15]:\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454db427-fd0f-4a62-a8d3-c4307fc2a390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pd.DataFrame(results_dict['mistralai/Mistral-Small-24B-Instruct-2501']).to_csv(\"27b_results_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5142615-75b2-4c2a-9487-27ac3222f29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564934d3-9957-4f7b-86af-5cad80108ea5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"testa\"[-1:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
